---
title: "Contrastive Pretraining of Regression Tasks in Reliability Forecasting of Automotive Electronics"
collection: publications
permalink: /publication/ssl-automotive
excerpt: 'This paper is about using contrastive learning in regression tasks by contrasting via Radial Basis Function of continuous labels.'
date: 2023-12-15
venue: '22nd International Conference on Machine Learning and Applications'
paperurl: 'https://ieeexplore.ieee.org/document/10460043'
citation: 'E. Zarbali et al. (2023). &quot;Contrastive Pretraining of Regression Tasks in Reliability Forecasting of Automotive Electronics.&quot; <i>2023 International Conference on Machine Learning and Applications (ICMLA)</i>. pp.332-338(1).'

---

Abstract
===
Predictive maintenance and reliability analysis have notoriously imperfect and unbalanced data because of few failure tests. This challenge limits the capability of supervised learning. Data-driven methods, especially deep learning, show great potential in capturing such complex relationships in many domains. Those methods rely heavily on the amount of data to extract meaningful representations. In contrast, data-poor domains depend on the use of simple architectures with a limited amount of trainable parameters. In this work, we show how pretraining with supervised contrastive learning can lead to better representations in data-poor domains such as the lifetime estimation on the example of solder joint. With such pre-trained models we are able to outperform end-to-end learning methods. Furthermore, we show that the learned representations improve classification tasks as well as rearession tasks.

Recommended citation: 'E. Zarbali et al. (2023). &quot;Contrastive Pretraining of Regression Tasks in Reliability Forecasting of Automotive Electronics.&quot; <i>2023 International Conference on Machine Learning and Applications (ICMLA)</i>. pp.332-338(1).'
